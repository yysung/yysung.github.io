[{"authors":["yoo-yeon-sung"],"categories":null,"content":"Hi, I\u0026rsquo;m Yoo Yeon (\u0026ldquo;You-yawn\u0026rdquo;)! I am a fifth-year Ph.D. candidate at the College of Information at University of Maryland, College Park. I am fortunate to be advised by Jordan Boyd-Graber and Naeemul Hassan. I got a M.S. degree in Industrial Management Engineering from Korea university, and B.A. degree in English Literature and Language from Chung-Ang University. I was a visiting researcher at KAIST AI Graduate school, advised by Jaegul Choo. My current research is in Human-Centered NLP and Responsible AI.\nRecent News February 2025: GRACE paper preprint is out! January 2025: AdvScore paper accepted to NAACL 2025 Main (Meta review score: 5/5)! This work was awarded the MetaAI Dynabench Grant: “A Leaderboard and Competition for Human–computer Adversarial Question Answering” November 2024: Hosted online QANTA data collection (Round 2). Official webpage November 2024: QB2NQ paper accepted to EMNLP 2024 Main! October 2024: Hosted in-person QANTA data collection at MIT and Berkeley for Human-grounded LLM calibration evaluation! Authoring interface May 2024: Started summer internship at Megagon Labs! November 2023: Not All Fake News is Written paper accepted to EMNLP 2023 Main! Research My research focuses on the human-centered evaluation of LLMs. Specifically, I work on creating benchmarks, developing evaluation metrics, and fine-tuning language models to better distinguish LLMs from humans or enhance human-AI complementarity. I enjoy conducting large-scale human user studies where humans interact with LLMs, enabling observing of how well LLMs jointly process tasks with humans, as humans, and assessing their ability to align with human reasoning, collaboration, and decision-making processes. By uncovering LLM vulnerabilities in comparison to humans, I aim to contribute to the development of safe language systems that align with human behavior and values.\nMy recent work include:\nCreating human-in-the-loop adversarial benchmarks to evaluate model capability and calibration Formulating human-grounded metrics to assess a benchmark’s adversarial robustness Developing a human-centered evaluation framework for accountable LLM agent systems Currently, I am working on:\nDesigning human-driven adversarial benchmarks to expose VLM vulnerabilities Personalizing LLMs for users in misinformation tasks Investigating how humans and LLMs cooperate in competitive environments The keywords that excite me the most are: Human-AI alignment, human-centered LLM evaluation, and AI robustness and reliability. Since human behavior and data are inherent to my research questions, I highly value a human-grounded approach to building, measuring, and interacting with language systems.\n","date":1740441600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1740441600,"objectID":"80f3ba9f8df1eced15b4ff7ca4119456","permalink":"/author/yoo-yeon-sung-%EC%84%B1%EC%9C%A0%EC%97%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yoo-yeon-sung-%EC%84%B1%EC%9C%A0%EC%97%B0/","section":"authors","summary":"Hi, I\u0026rsquo;m Yoo Yeon (\u0026ldquo;You-yawn\u0026rdquo;)! I am a fifth-year Ph.D. candidate at the College of Information at University of Maryland, College Park. I am fortunate to be advised by Jordan Boyd-Graber and Naeemul Hassan.","tags":null,"title":"Yoo Yeon Sung (성유연)","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"42bc04c138a3c5b43479eff9fa12bf11","permalink":"/home-unused/slider/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/slider/","section":"home-unused","summary":"","tags":null,"title":"","type":"home-unused"},{"authors":null,"categories":null,"content":"The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)\nBuild Anything with Widgets\nStar\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ff3af16109a08648bdce10d3973de99c","permalink":"/home-unused/hero/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/hero/","section":"home-unused","summary":"The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)\nBuild Anything with Widgets\nStar","tags":null,"title":"Academic","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4a7e3501655fed0a4b0ce814e15ff2c9","permalink":"/home-unused/skills/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/skills/","section":"home-unused","summary":"","tags":null,"title":"Skills","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d6682c06ff2f3dd0fc28f7e2c0702d07","permalink":"/home-unused/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/experience/","section":"home-unused","summary":"","tags":null,"title":"Experience","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9e909a8894fd21a2eff4b3e43238d81e","permalink":"/home-unused/accomplishments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/accomplishments/","section":"home-unused","summary":"","tags":null,"title":"Accomplish\u0026shy;ments","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0e643989bdefe366f2b5fddf949a36b6","permalink":"/home-unused/posts/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/posts/","section":"home-unused","summary":"","tags":null,"title":"Recent Posts","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"89201c04ad04664a30c3fb9ba7b170aa","permalink":"/home-unused/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/projects/","section":"home-unused","summary":"","tags":null,"title":"Projects","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b3f45f4a1c65dae9e5e30f53b9f83edd","permalink":"/home-unused/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/people/","section":"home-unused","summary":"","tags":null,"title":"Meet the Team","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d927b251d3da15a737d1f66fb88d4504","permalink":"/home-unused/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/talks/","section":"home-unused","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"28f54f6e819207239a6024bbaa9d78de","permalink":"/home-unused/featured/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/featured/","section":"home-unused","summary":"","tags":null,"title":"Featured Publications","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"657179738bed56748434d6ae76e8a647","permalink":"/home-unused/tags/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/tags/","section":"home-unused","summary":"","tags":null,"title":"Popular Topics","type":"home-unused"},{"authors":["Yoo Yeon Sung (성유연)","Hannah Kim","Dan Zhang"],"categories":null,"content":"","date":1740441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740441600,"objectID":"f40ec22375eb7366cc61d5eacd9ddcd8","permalink":"/publication/advscore-a-metric-for-the-evaluation-and-creation-of-adversarial-benchmarks-copy-2/","publishdate":"2025-02-25T00:00:00Z","relpermalink":"/publication/advscore-a-metric-for-the-evaluation-and-creation-of-adversarial-benchmarks-copy-2/","section":"publication","summary":"AI practitioners increasingly use large language model (LLM) agents in compound AI systems to solve complex reasoning tasks, these agent executions often fail to meet human standards, leading to errors that compromise the system's overall performance. Addressing these failures through human intervention is challenging due to the agents' opaque reasoning processes, misalignment with human expectations, the complexity of agent dependencies, and the high cost of manual inspection. This paper thus introduces a human-centered evaluation framework for Verifying LLM Agent failures (VeriLA), which systematically assesses agent failures to reduce human effort and make these agent failures interpretable to humans. The framework first defines clear expectations of each agent by curating human-designed agent criteria. Then, it develops a human-aligned agent verifier module, trained with human gold standards, to assess each agent's execution output. This approach enables granular evaluation of each agent's performance by revealing failures from a human standard, offering clear guidelines for revision, and reducing human cognitive load. Our case study results show that VeriLA is both interpretable and efficient in helping practitioners interact more effectively with the system. By upholding accountability in human-agent collaboration, VeriLA paves the way for more trustworthy and human-aligned compound AI systems.","tags":null,"title":"VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures","type":"publication"},{"authors":["Yoo Yeon Sung (성유연)","Eve Fleisig","Yu Hope","Ishan Upadhyay","and Jordan Boyd-Graber"],"categories":null,"content":"","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"2e9ddf94d866e42f6e4493db10181d70","permalink":"/publication/advscore-a-metric-for-the-evaluation-and-creation-of-adversarial-benchmarks-copy/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/publication/advscore-a-metric-for-the-evaluation-and-creation-of-adversarial-benchmarks-copy/","section":"publication","summary":"Language models are often miscalibrated, leading to confidently incorrect answers. We introduce GRACE, a benchmark for language model calibration that incorporates comparison with human calibration. GRACE consists of question-answer pairs, in which each question contains a series of clues that gradually become easier, all leading to the same answer; models must answer correctly as early as possible as the clues are revealed. This setting permits granular measurement of model calibration based on how early, accurately, and confidently a model answers. After collecting these questions, we host live human vs. model competitions to gather 1,749 data points on human and model teams' timing, accuracy, and confidence. We propose a metric, CalScore, that uses GRACE to analyze model calibration errors and identify types of model miscalibration that differ from human behavior. We find that although humans are less accurate than models, humans are generally better calibrated. Since state-of-the-art models struggle on GRACE, it effectively evaluates progress on improving model calibration.","tags":null,"title":"Granular Benchmark for Evaluating Model Calibration against Human Calibration","type":"publication"},{"authors":["Yoo Yeon Sung (성유연)","Maharshi Gor","Eve Fleisig","Ishani Mondal","and Jordan Boyd-Graber"],"categories":null,"content":"","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"88dc38d84f0603ade683ff308524bb99","permalink":"/publication/advscore-a-metric-for-the-evaluation-and-creation-of-adversarial-benchmarks/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/publication/advscore-a-metric-for-the-evaluation-and-creation-of-adversarial-benchmarks/","section":"publication","summary":"Adversarial datasets should ensure AI robustness that matches human performance. However, as models evolve, datasets can become obsolete. Thus, adversarial datasets should be periodically updated based on their degradation in adversarialness. Given the lack of a standardized metric for measuring adversarialness, we propose AdvScore, a human-grounded evaluation metric. AdvScore assesses a dataset's true adversarialness by capturing models' and humans' varying abilities, while also identifying poor examples. AdvScore then motivates a new dataset creation pipeline for realistic and high-quality adversarial samples, enabling us to collect an adversarial question answering (QA) dataset, AdvQA. We apply AdvScore using 9,347 human responses and ten language model predictions to track the models' improvement over five years (from 2020 to 2024). AdvScore assesses whether adversarial datasets remain suitable for model evaluation, measures model improvements, and provides guidance for better alignment with human capabilities.","tags":null,"title":"Is your benchmark truly adversarial? AdvScore: Evaluating Human-Grounded Adversarialness","type":"publication"},{"authors":["Tasnim Kabir","Yoo Yeon Sung (성유연)","Saptarashmi Bandyopadhyay","Hao Zou","Abhranil Chandra","and Jordan Lee Boyd-Graber"],"categories":null,"content":"","date":1729814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729814400,"objectID":"e332be1f0c6c6099bcbf743a9a4982ce","permalink":"/publication/you-make-me-feel-like-a-natural-question-training-qa-systems-on-transformed-trivia-questions/","publishdate":"2024-10-25T00:00:00Z","relpermalink":"/publication/you-make-me-feel-like-a-natural-question-training-qa-systems-on-transformed-trivia-questions/","section":"publication","summary":"Training question-answering QA and information retrieval systems for web queries require large, expensive datasets that are difficult to annotate and time-consuming to gather. Moreover, while natural datasets of information-seeking questions are often prone to ambiguity or ill-formed, there are troves of freely available, carefully crafted question datasets for many languages. Thus, we automatically generate shorter, information-seeking questions, resembling web queries in the style of the Natural Questions (NQ) dataset from longer trivia data. Training a QA system on these transformed questions is a viable strategy for alternating to more expensive training setups showing the F1 score difference of less than six points and contrasting the final systems.","tags":null,"title":"You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions","type":"publication"},{"authors":["Yoo Yeon Sung (성유연)","Jordan Boyd-Graber","and Naeemul Hassan"],"categories":null,"content":"","date":1674864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674864000,"objectID":"c29fbec4c00454566165a1fbc08199fc","permalink":"/publication/not-all-fake-news-is-written-a-dataset-and-analysis-of-misleading-video-headlines/","publishdate":"2023-01-05T00:00:00Z","relpermalink":"/publication/not-all-fake-news-is-written-a-dataset-and-analysis-of-misleading-video-headlines/","section":"publication","summary":"Polarization and the marketplace for impressions have conspired to make navigating information online difficult for users, and while there has been a significant effort to detect false or misleading text, multimodal datasets have received considerably less attention. To complement existing resources, we present multimodal Video Misleading Headline (VMH), a dataset that consists of videos and whether annotators believe the headline is representative of the video's contents. After collecting and annotating this dataset, we analyze multimodal baselines for detecting misleading headlines. Our annotation process also focuses on why annotators view a video as misleading, allowing us to better understand the interplay of annotators' background and the content of the videos.","tags":null,"title":"Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines","type":"publication"},{"authors":["Yoo Yeon Sung (성유연)","Ishani Mondal","and Jordan Boyd-Graber"],"categories":null,"content":"","date":1674172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674172800,"objectID":"dafed33703b955b148a5e8f45d8266b5","permalink":"/publication/how-the-advent-of-ubiquitous-large-language-models-both-stymie-and-turbocharge-dynamic-adversarial-question-generation/","publishdate":"2023-01-20T00:00:00Z","relpermalink":"/publication/how-the-advent-of-ubiquitous-large-language-models-both-stymie-and-turbocharge-dynamic-adversarial-question-generation/","section":"publication","summary":"Dynamic adversarial question generation, where humans write examples to stump a model, aims to create examples that are realistic and informative. However, the advent of large language models (LLMs) has been a double-edged sword for human authors: more people are interested in seeing and pushing the limits of these models, but because the models are so much stronger an opponent, they are harder to defeat. To understand how these models impact adversarial question writing process, we enrich the writing guidance with LLMs and retrieval models for the authors to reason why their questions are not adversarial. While authors could create interesting, challenging adversarial questions, they sometimes resort to tricks that result in poor questions that are ambiguous, subjective, or confusing not just to a computer but also to humans. To address these issues, we propose new metrics and incentives for eliciting good, challenging questions and present a new dataset of adversarially authored questions.","tags":null,"title":"How the Advent of Ubiquitous Large Language Models both Stymie and Turbocharge Dynamic Adversarial Question Generation","type":"publication"},{"authors":["Yoo Yeon Sung and Seoung Bum Kim"],"categories":null,"content":"","date":1580169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580169600,"objectID":"b93bb640d9f7d59daa9d5005769a8633","permalink":"/publication/topical-keyphrase-extraction-with-hierarchical-semantic-networks/","publishdate":"2020-01-05T00:00:00Z","relpermalink":"/publication/topical-keyphrase-extraction-with-hierarchical-semantic-networks/","section":"publication","summary":"Topical keyphrase extraction is used to summarize large collections of text documents. However, traditional methods cannot properly reflect the intrinsic semantics and relationships of keyphrases because they rely on a simple term-frequency-based process. Consequently, these methods are not effective in obtaining significant contextual knowledge. To resolve this, we propose a topical keyphrase extraction method based on a hierarchical semantic network and multiple centrality network measures that together reflect the hierarchical semantics of keyphrases. We conduct experiments on real data to examine the practicality of the proposed method and to compare its performance with that of existing topical keyphrase extraction methods. The results confirm that the proposed method outperforms state-of-the-art topical keyphrase extraction methods in terms of the representativeness of the selected keyphrases for each topic. The proposed method can effectively reflect intrinsic keyphrase semantics and interrelationships.","tags":null,"title":"Topical Keyphrase Extraction with Hierarchical Semantic Networks","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1da304e9ea1b167ac9baed030c02b693","permalink":"/talk/nottalk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/nottalk/","section":"talk","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"talk"}]